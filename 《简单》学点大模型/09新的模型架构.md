# 新的模型架构

GPT-3是一个通过堆叠96层Transformer block，映射token映射token序列$x_{1:L}$的神经语言模型

其中，每层Transformer block使用
- 自注意力层，允许每个token进行交互
- 前馈层，独立处理每个token：

这是开发大语言模型的主要范式,但扩展需要数据、模型和流水并行。

现状是，模型到达了极限，并且随着模型的提升需要拆分到更多机器。而根据上一章节的分布式训练得知，这需要网络数据传输。拆分的越多，网络负担也越大，成为训练的瓶颈。

总结：当力大砖飞不再可行时，应追求新的模型架构。

例如：让每个输入使用不同但更小的参数子集？（实际上自己不知道那是什么）

这一章讨论了两种不同类型的“新”模型架构，以应对力大砖飞所遇到的瓶颈。

- 混合专家模型：我们创建一组专家。每个输入只激活一小部分专家。
- 基于检索的模型：给定一个新的输入，我们检索存储库中和它相关的部分，并使用它们来预测输出。

实际上这两种都是让输入使用更小参数子集的一种做法。

## 混合专家模型

当我们在解决一个预测问题时

$$
x \in \mathbb{R}^d \Rightarrow y \in \mathbb{R}^d.
$$

这里有一个学习前馈（ReLU）神经网络：

$$
h_\theta(x) = W_2 \max(W_1 x, 0),
$$

其中参数为 $\theta = (W_1, W_2)$。

这个问题在于，函数的表达能力可能不足，可以用更宽或者更深的神经网络。

专家的混合方法是：

- 定义 $E$ 个专家
- 每个专家$e = 1, \dots, E$都具有自己的嵌入$w_e \in \mathbb{R}^d$。
- 将门控函数定义为$E$个专家上的概率分布：

$$
g_e(x) = \frac{\exp(w_e \cdot x)}{\sum_{e' = 1}^E \exp(w_{e'} \cdot x)}.
$$

- 每个专家$e = 1, \dots, E$都具有自己的参数$\theta^{(e)} = (W_1^{(e)}, W_2^{(e)})$。

- 根据专家特定参数定义每个专家函数：

$$
h_{\theta_e}(x) = W_2^{(e)} \max(W_1^{(e)} x, 0).
$$

- 将最终函数定义为专家的混合：

$$
f(x) = \sum_{e=1}^E \underbrace{g_e(x)}_\text{gating} \underbrace{h_{\theta_e}(x)}_\text{expert}.
$$

公式概括的非常精炼，他的做法是让每个专家进行决策然后将结果**加权混合累加**，

训练时通过反向传播来进行训练，梯度同时影响到门控函数和专家。

### 节约计算

门控函数对每个专家来说都是非零的，例如：

$$
g(x) = [0.04, 0.8, 0.01, 0.15].
$$

也因此会带来极大的计算量……

因此可以考虑取前几名的专家，然后其他的专家置零来进行规范化，也就是：

$$
\tilde g(x) = [0, 0.84, 0, 0.16].
$$

别忘了让数组总和为1，**别怪我没警告过你**。

### 平衡专家

实际上只有一个专家的独裁根本不是“混合专家”。

所以尽量确保所有的专家能被输入使用。

但这可能和节约计算有冲突，不过两个专家肯定比一个专家独裁来的好。

### 并行

- 混合专家非常有利于并行。
- 每个专家都可以放置在不同的机器上。
- 我们可以在中心节点计算近似门控函数$\tilde g(x)$。
- 然后，我们只要求包含激活专家的机器（稀疏）来处理$x$。

这就是为“分布式训练”量身定做的策略。

之后作者讲述了很多模型的例子来验证使用了混合专家策略之后能给模型带来多大的提升。

因为有效所以使用。

## 基于检索的模型

这是另一种解法，基于检索的模型用于突破稠密Transformer的缩放上限。

### 编码器-解码器

对于编码器-解码器的任务来说：

$$
\text{input } x \quad\Rightarrow\quad \text{output } y
$$

他们使用去噪目标函数进行训练，这在之前的章节中有讲解。

### 检索方法

假设我们有一个存储库 $S$ ，它是一组序列的集合。

$$
S = \{ \text{Why is the...}, \text{Thanks for}, ..., \text{The quick...}, \text{Stanford...} \}.
$$

生成过程是：

- 基于输入 $x$ ，检索相关序列 $z$
- 给定检索序列$z$和输入$x$，生成输出$y$。

最近邻是最常用的一种检索方法：

- $S$是训练集。
- 检索$(x',y') \in S$，使得$x'$和$x$最相似。
- 生成$y = y'$。

### RAG（Retrieval-augmented generation）

RAG的模型定义如下：

$$
(y \mid x) = \sum_{z \in S} \underbrace{p(z \mid x)}_\text{retriever} \underbrace{p(y \mid z, x)}_\text{generator}.
$$

在实践中，$\sum_{z \in S}$由前k个代替（类似于为混合专家选择前1个或2个专家）。

仍然是精炼的概括。

#### 检索器

$$
p(z \mid x) = \frac{\exp(\text{BERT}_\text{d}(z) \cdot \text{BERT}_\text{q}(x))}{\sum_{z' \in S} \exp(\text{BERT}_\text{d}(z') \cdot \text{BERT}_\text{q}(x))}.
$$

#### 生成器

$$
p(y \mid z, x) = p(y \mid \text{concat}(z, x)).
$$

#### 训练

- 用BART、DPR（用BERT初始化）初始化
- 训练$\text{BART}$和$\text{BERT}_\text{q}$

然后作者通过RETRO的实验来表明检索模型很适合知识密集型的任务。
