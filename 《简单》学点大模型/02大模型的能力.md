# 大模型能力

本章节关于GPT-3的能力解析

GPT-3是通用的语言大模型，适合广泛的NLP任务，但在部分任务上表现落后。

为此，应该利用大量的标签数据来适应GPT-3，并超越当前的技术水平。

## 2.1 语言模型的适应性：从语言模型到任务模型的转化

任务：从输入映射到输出。

例如：

> input:
>  What school did Burne Hogarth establish?
> 
> output:
> School of Visual Arts
> 
> Real GPT-3:
> Burne Hogarth did not establish a school, but he was a renowned American cartoonist and illustrator who is best known for his work in the comic strip "Tarzan" and for his instructional books on art and anatomy, particularly "Dynamic Anatomy" and "Dynamic Figure Drawing." While he didn't establish a school, his books and teachings have been influential in the world of art and have inspired many aspiring artists and illustrators. Hogarth's emphasis on dynamic and expressive figure drawing has had a lasting impact on the field of comic art and illustration.

嗯，我测试了一下真实的GPT-3的输出，看来生成的和我想得不太一样。

“适应（Adaptation）”指代将语言模型转化为任务模型的过程。这个过程需要以下两个输入：

1. 任务的自然语言描述
2. 一组训练实例（输入-输出对）

为了进行“适应”，我们可以做：

- 训练：包括训练新模型利用语言模型作为特征（探针法），或者工具训练示例进行更新（微调），亦或者取两者之间。
- 提示：根据描述和训练实例构建一个或一组提示，将其输入到语言模型中以获取完成的序列。


对于GPT-3来说，我们的提示和完成只有2048tokens的机会，这也是我遇到为什么在大量文本输入的情况下会忘记上文的原因。

为了评估GPT-3的表现，他们会讨论

- 定义
- 适应
- 结果

于是作者评估了六类不同的任务

### 2.1.1 Language Modeling

大语言模型的基本任务，询问语言是否完成了本职的工作。

假设我们有一段文本 $x_{1:L}$ ，例如：

$$
\text{𝗍𝗁𝖾 𝗆𝗈𝗎𝗌𝖾 𝖺𝗍𝖾 𝗍𝗁𝖾 𝖼𝗁𝖾𝖾𝗌𝖾}
$$


我们可以询问：语言模型会给这段文本分配什么概率？

$$
p(\text{𝗍𝗁𝖾 𝗆𝗈𝗎𝗌𝖾 𝖺𝗍𝖾 𝗍𝗁𝖾 𝖼𝗁𝖾𝖾𝗌𝖾})
$$

联合概率可以分解为每个流派的条件概率乘积，是通过链式法则完成的（仍然是连乘）：

$$
p(x_{1:L}) = \prod_{i=1}^L p(x_i \mid x_{1:i-1}).
$$

困惑度是非常重要的指标，用于衡量语言模型的性能。值越低代表预测越准确，定义式为：

$$
P(X) = P(x_1,x_2,...,x_N)^{(-1/N)}
$$

其中， $X=x_{1},x_{2},...,x_{N}$ 是测试集中的词序列， $N$ 是测试集中的总词数。困惑度与语言模型的质量紧密相关。

通常来说，优秀的模型能准确预测数据中的词序列，因此困惑值较低。

一个序列的联合概率取决于其长度，越长越趋近于0（甚至会下溢）。直观上，我们希望对每个词标记（token）的概率 $p(x_{i}∣x_{1:i−1})$ 进行平均。

采用几何平均，而不是算数平均，为防止**极低概率**在受到平均之后会变成**高概率**。


```math
\text{perplexity}_p\left(x_{1: L}\right)=\exp \left(\frac{1}{L} \sum_{i=1}^L \log \frac{1}{p\left(x_i \mid x_{1: i-1}\right)}\right) \text {. }
```

困惑度可以被理解为每个标记的平均“分支因子”，可理解为在每个特定的词或者标记后，下一个词或标记可能的词或标记的平均数量。

我能从公式中看到香农定理的影子，很明显就是这个 $\log \frac{1}{p\left(x_i \mid x_{1: i-1}\right)}$ 密切相关，它就代表了编码长度。我们在计算的是平均编码长度，这个长度反映了给定当前词或标记后，下一个词或标记可能的选择数量。因此，通过对平均编码长度取指数，我们可以得到可能的选择数量，这也就是"分支因子"。

那么也可以说：选择可能性越多，模型预测任务越复杂，则困惑度越高。

补充举例：

1. **句子结构的复杂性：** 假设有两个句子，一个是简单的陈述句，另一个是复杂的嵌套句子。例如，考虑以下两个句子：

   - 简单句子： "The sky is blue."
   - 复杂句子： "The cat that chased the mouse that ate the cheese is on the roof."

   对于一个语言模型来说，预测简单句子的下一个词相对容易，因为句子结构简单明了。但是，对于复杂句子，模型需要考虑多个可能的嵌套结构和下一个词的选择，这会增加困惑度。

2. **多义词和歧义性：** 当输入包含多义词或存在歧义时，语言模型往往会更加困惑。例如，考虑以下句子：

   - "I saw a man with a telescope."
   - "He plays bass guitar."

   在第一个句子中，"with a telescope"可以解释为是男人用望远镜看东西，也可以解释为男人本身携带了望远镜。在第二个句子中，"bass"可以是低音吉他的意思，也可以是鱼的种类。这种多义性增加了模型在预测下一个词时的不确定性，导致困惑度增加。

3. **词汇量的增加：** 当语言模型需要处理更大的词汇量时，通常会增加困惑度。例如，在一个医学文档中，模型可能会面临大量的医学术语和专业名词，这些名词不太常见，因此模型在预测它们时会更加困惑。

语言模型可能会犯两类错误，**召回错误**和**精度错误**：

**召回错误**：语言模型未能正确为某个词分配概率值。例如，如果模型为词组 '𝖺𝗍𝖾' 在 '𝗍𝗁𝖾,𝗆𝗈𝗎𝗌𝖾' 后出现的概率预测为接近0，那么对应的困惑度值将趋近于无穷大。

**精确度错误**：语言模型为某些错误的词序列过度分配了概率值。在这种情况下，困惑度会进行适度的惩罚。给定一个语言模型 p，假设我们将一些垃圾分布 $r$ 按照概率 $ϵ$ 混入：

$$
q(x_i \mid x_{1:i-1}) = (1-\epsilon) p(x_i \mid x_{1:i-1}) + \epsilon r(x_i \mid x_{1:i-1}).
$$

那么，我们可以计算在 $q$ 下的 $x_{1:L}$ 的困惑度：

```math
\text{perplexity}_q(x_{1:L}) \le \frac{1}{1 - \epsilon} \text{perplexity}_p(x_{1:L}) \approxeq (1 + \epsilon) \text{perplexity}_p(x_{1:L}),
```

那就是无意义或者垃圾的信息混入就会让生成结果变得非常糟糕，哪怕只有5%。

这两个错误都是给词分配了错误的概率，一个是过于接近0，另一个是混入了错误的分布。

失之毫厘，谬以千里。

#### 2.1.1.1 Penn Tree Bank

作者使用这种经典的数据集 作为提示输入到GPT-3中，并进行困惑度评估。

即便GPT-3比Perplexity表现更佳，但是有可能有数据泄露问题，也就是训练集的数据可能已经被用于GPT-3的训练中了。

所以这个可能并不足够评价模型的能力。

#### 2.1.1.2 LAMBADA

任务是预测句子的最后一个词，这个任务要求对较长内容进行建模并对内容具有一定依赖。

我们可以看一下这个示例：
```
Fill in blank:  
  
Alice was friends with Bob. Alice went to visit her friend ___. -> Bob  
  
She held the torch in front of her.  
She caught her breath.  
“Chris? There’s a step.”  
“What?”  
“A step. Cut in the rock. About fifty feet ahead.” She moved faster. They both moved faster. “In fact,” she said, raising the torch higher, “there’s more than a ___. -> step
```

分析第一个例子，它必须识别上文“Alice was friends with Bob”，否则不知道她要去访问哪个朋友。

第二个例子相对较长，上文或者关键词在“A step.”上，句子的意思是让她注意到台阶，实际上这个关键词是有一些距离的，很考验大模型的能力。

还是一样GPT击败了SOTA……

#### 2.1.1.3 HellaSwag

从一系列选择中选择最合适的完成句子选项，是一个多项选择任务，预测最佳答案。

```
Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They ${answer}
```

其中 ${answer} 是以下选项之一：
1. _bake them, then frost and decorate._
2. _taste them as they place them on plates._
3. _put the frosting on the cake as they pan it._
4. _come out and begin decorating the cake as well._

真的如同中学做那种阅读理解那样，其实也是那样，以我英语六级的水平解释这个题目（实际上是GPT-3翻译的）：

制作蛋糕：展示了几个蛋糕棒在一个展示上。一个女人和一个女孩在厨房里制作蛋糕棒。他们会？

1. 烘焙、撒糖霜并装饰
2. 当他们摆盘之后吃掉它！
3. 当他们把蛋糕从锅里拿出来时上糖霜。
4. 拿出来并开始装饰蛋糕。


其实这就是按照一个常理去选择最合理的选项，朵朵的选择是1.

给定问题 x ，对答案 y 进行评分，通常有一些启发方法：

- 未归一化的概率：$score(x,y)=p(x,y)$ 。未归一化概率的问题是它倾向于短答案。
- 长度归一化的概率：$score(x,y)=p(x,y)/num-tokens(y)$ 。这修正了长度偏见。然而，对于长度相同的两个答案，模型仍可能偏好更受欢迎的实体。
- 频率归一化的概率：$score(x,y)=p(y∣x)/p(y∣x_{0})$ ，其中 $x_{0}$ 是一个中立的字符串，如'Answer:'。这降低了恰巧很常见的答案（例如，“John”）的得分。

但是GPT-3被SOTA微微击败，但很明显GPT-3在**不经过微调**的情况下能达到很接近的效果，说明也已经足够厉害了。

### 2.1.2 Question answering

#### 2.1.2.1 TriviaQA

回答问题，很简单。是业余爱好者收集的问题。

在这类问题中，增加模型大小和增加in-context training实例都有助于提高性能。

#### 2.1.2.2 WebQuestions

和上面不同的地方在于数据集从Google搜索查询中收集。


```
Q: What school did burne hogarth establish?  
A: School of Visual Arts
```

结果是GPT-3需要一些预训练才能与RAG相抗衡。

#### 2.1.2.3 NaturalQuestions

和上面的又一个区别是答案更长了

```
Q: Who played tess on touched by an angel?  
A: Delloreese Patricia Early (July 6, 1931 - November 19, 2017), known professionally as Della Reese.
```

结果很明显，RAG碾压了经过预训练的GPT-3.

### 2.1.3 Translation

这是一个机器翻译任务，他们测试的是从德语翻译成英语。

实际上不论是它的结果还是我测试的结果，翻译效果GPT-3都表现的非常好，甚至超过了SOTA（监督训练）。

作者的结果是：

- 即使没有监督训练数据，GPT-3也能达到全监督系统的最新技术水平！
- 这为机器翻译的性能设定了一个下限；因为肯定会想利用大量的平行语料库（对齐的输入-输出对）。
- 法语和罗马尼亚语的结果类似。
- 从英语到外语的结果要差得多，这是可以预料的，因为GPT-3主要是一个英语模型。

但我一般用于中英文之间的互译，不过自己不是专门的翻译使用用户，因此对其要求没那么高。

### 2.1.4 Arithmetic

实际上就是做数学问题，我个人对它的数学表现并不满意，尤其是有一定复杂度的数学问题时。我可以当场出一道问题来做：

> 一个不超过105的正整数，除以3余2，除以5余1，除以7余4，求这个数。


GPT-3输出是

```
要找到一个满足以上条件的正整数，可以使用中国剩余定理来解决这个问题。首先，考虑满足每个余数条件的整数：

1. 除以3余2的整数形式为 3k + 2。
2. 除以5余1的整数形式为 5m + 1。
3. 除以7余4的整数形式为 7n + 4。

接下来，我们可以将这些条件合并为一个等式：

3k + 2 = 5m + 1 = 7n + 4

我们可以将其中一个条件表示为等于其他两个条件的形式。让我们从第一个和第二个条件开始：

3k + 2 = 5m + 1

现在，我们可以看到，当 k = 3 时，这两个条件相等。因此，我们可以将它们简化为：

k = 3, 5m + 1 = 2

现在，我们可以看到 m = 0 是满足第二个条件的一个解。所以，现在我们有：

k = 3, m = 0

接下来，我们可以将这些值代入第三个条件：

7n + 4 = 3(3) + 2

7n + 4 = 9 + 2

7n + 4 = 11

现在，解出 n：

7n = 11 - 4
7n = 7

n = 1

现在我们有 k = 3，m = 0，n = 1。为了找到最小的正整数，我们可以将它们组合起来：

k = 3, m = 0, n = 1

现在，我们可以使用这些值来计算最小的正整数：

正整数 = 3k = 3(3) = 9

所以，满足这些条件的最小正整数是 9。
```

经典的韩信点兵问题，给我干沉默了！

其中一个可行解是 11

- 11%3=2
- 11%5=1
- 11%7=4


对于韩信点兵问题，我尝试使用英语提问：

```
A positive number below 105. Which mod 3 equals 2 and mod 5 equals 1 and mod 7 equals 4. Pleasse give me this number.
```

GPT-3输出

```
To find a positive number below 105 that satisfies the given conditions, you can use the Chinese Remainder Theorem (CRT). 

The Chinese Remainder Theorem states that if you have a system of congruences like the ones you've provided, you can find a unique solution modulo the product of the moduli (in this case, 3 * 5 * 7 = 105).

Let's call the number you're looking for "x." We have three congruences:

1. x ≡ 2 (mod 3)
2. x ≡ 1 (mod 5)
3. x ≡ 4 (mod 7)

First, find the modular inverses of 105 divided by each of the moduli:

1. Modular inverse of 105 (mod 3) = 2 (because 2 * 105 ≡ 1 (mod 3))
2. Modular inverse of 105 (mod 5) = 1 (because 1 * 105 ≡ 1 (mod 5))
3. Modular inverse of 105 (mod 7) = 4 (because 4 * 105 ≡ 1 (mod 7))

Now, apply the Chinese Remainder Theorem:

x = (2 * 2 * 2 * 1 + 1 * 1 * 21 * 4 + 4 * 4 * 15 * 1) % 105

x = (8 + 84 + 60) % 105

x = 152 % 105

x = 47

So, the positive number below 105 that satisfies the given conditions (mod 3 equals 2, mod 5 equals 1, and mod 7 equals 4) is 47.
```

- 47%3=2
- 47%5=2
- 47%7=5

实际上即便是用英文也在瞎说，根本不适合做有一定复杂度的数学题的！


作者测试的是更加简单的算术题，2-5位的加减乘，同样也是不满意的。

### 2.1.5 News article generation

编东西AI最擅长了，上面他在瞎说，这里正好就是给它瞎说的地方。

任务：给定标题和副标题，生成新闻文章。
数据集：标题/副标题取自[newser.com]

提示词只需要描述任务是什么就可以了。

另一个任务是纠正英语语法，这个肯定也是AI擅长的，从原理就可以知道句式合理会被赋予一个更高的概率，使得其更加倾向于合理的句子结构和单词组合。

我们再试一下还是那个例子，我可能表达的有些问题，优化一下语法。

>Please fix language problems for following sentence.
>A positive number below 105. Which mod 3 equals 2 and mod 5 equals 1 and mod 7 equals 4. Pleasse give me this number.

>Certainly! Here's the corrected sentence:
>
>"Please provide me with a positive number below 105 that satisfies the following conditions: It leaves a remainder of 2 when divided by 3, a remainder of 1 when divided by 5, and a remainder of 4 when divided by 7."

所以可以尝试一下将问题重新描述之后再次投喂给GPT来寻找答案。

```
To find a positive number that satisfies these conditions, you can use the Chinese Remainder Theorem. 

First, let's consider the remainders:
1. A remainder of 2 when divided by 3 implies the number can be represented as 3a + 2.
2. A remainder of 1 when divided by 5 implies the number can be represented as 5b + 1.
3. A remainder of 4 when divided by 7 implies the number can be represented as 7c + 4.

Now, we can combine these conditions into a single equation:
3a + 2 = 5b + 1 = 7c + 4

To find a positive number that satisfies this equation, you can start with the second part: 5b + 1 = 7c + 4. Solve for b:
5b = 7c + 3
b = (7c + 3) / 5

Now, you need to find values of c for which (7c + 3) is divisible by 5. Let's try different values of c:
- When c = 0, (7c + 3) = 3, which is not divisible by 5.
- When c = 1, (7c + 3) = 10, which is divisible by 5.

So, when c = 1, b = 10 / 5 = 2.

Now that you have b = 2, you can find a by plugging it back into the first part of the equation:
3a + 2 = 5b + 1
3a + 2 = 5(2) + 1
3a + 2 = 10 + 1
3a + 2 = 11

Now, solve for a:
3a = 11 - 2
3a = 9
a = 9 / 3
a = 3

So, a = 3, b = 2, and c = 1. Now, you can find the number that satisfies all the conditions:
3a + 2 = 3(3) + 2 = 9 + 2 = 11

Therefore, the positive number that satisfies all the conditions is 11.
```

GPT-3回答正确
所以这是一种可行的策略。

### 2.1.8 总结
- GPT-3在广泛的标准NLP基准测试和一次性任务上进行了评估。
- GPT-3可以表现得极好或者非常普通。
- 增加模型的大小和示例的数量都有助于提高性能。
- 有一些启发式的方法可以将语言模型适应到感兴趣的任务。
- 但是为什么会有这样表现，没有人知道。
- 所以仍然要多保持探索，比如这个最经典的韩信点兵问题，能够挖掘很多很好玩的地方。