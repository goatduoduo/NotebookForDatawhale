大模型是一个黑箱

给定一个基于自身需求的prompt就可以生成符合需求的结果

可表示为：

$$
prompt \leadsto completion
$$

从数学角度考虑对训练数据的概率分布：

$$
trainingData \Rightarrow p(x_{1},...,x_{L}).
$$

本章节关于如何构建大语言模型。

- 分词：如何将一个字符串拆分成多个标记
- 模型架构：Transformer架构，真正实现了大语言模型的创新


## 分词

语言模型 $p$是一个对标记（token）序列的概率分布，其中每个标记来自某个词汇表$V$：

$$ [the, mouse, ate, the, cheese] $$

但是，自然语言并不会以“标记序列”的方式出现，而是字符串的形式存在，例如"the mouse ate the cheese"。

分词器将“字符串”转换为“序列”

the mouse ate the cheese $\Rightarrow [the, mouse, ate, the, cheese]$

### 基于空格分词

使用空格进行分词，适用于英文，但对一些语言并不适用！

- 中文：我今天吃胡萝卜。
- 德语：Abwasserbehandlungsanlange 长复合词
- 英语：连字符词father-in-law 或者 don't

在这些问题下，我们决定探索更好的分词系统。

- 限制标记数量，否则极难建模。
- 确保足够的标记，部分单词之间应该要共享参数，尤其是形态丰富的语言。
- 每个标记是一个语言或者统计上的有意义单位。

### Byte pair encoding 

将字节对编码算法应用于数据压缩领域，用于生成其中一个最常用的分词器。

当然这种分词器是需要进行学习的，通过模型数据进行训练，获得分词文本的频率特征。

对于分词器的学习过程：

- 输入：训练语料库
- 初始化词汇表$V$为字符的集合
- 找到$V$中共同出现次数最多的元素对$x,x'$
- 用一个新的符号$xx'$替换所有$x,x'$的出现。将
- $xx'$添加到V中。

解释一下教程里的例子：

$1[t, h, e, c, c, a, r],[t, h, e, c, c, a, t],[t, h, e, c, r, a, t]$
2 [th, e, $, c, a, r],[t h, e, u, c, a, t],[t h, e, c, r, a, t]$ (th 出现了 3次)
3 [the, $\leq, c, a, r]$, [the, $\sqcup, c, a, t],[$ the, $u, r, a, t]$ (the 出现了 3次)
4 [the, $\sqcup, c a, r],[$ the, $u, c a, t],[$ the, $\sqcup, r, a, t]$ (ca 出现了 2次)

1->2 这三个序列"theccar","theccat","thecrat"中，出现最多的是 th ，尽管我们很明显看得出来 the最多，但不能一口气吃成大胖子，所以我们把th组成一起，就变成了 2 的样子

2->3 很明显the出现的更多，th和e组合起来

但是例子有问题，为什么1->2会莫名出现u，然后后面步骤还有奇怪的符号，这发生了什么？

#### Unicode的问题

Unicode指的是字符串在计算器存储的编码方式。但是这个字符非常多，尤其是涉及到大量不同语言的字符，中文就更加多了。

训练过程不可能覆盖到所有的字符，尤其是中文的生僻字，为了不那么稀疏，可以对字节而不是Unicode字符进行BPE算法。

以中文为例：
$$
\text { 今天} \Rightarrow \text {[x62, x11, 4e, ca]}
$$

这样做可以在多语言环境里更好处理Unicode字符的多样性，并减少低频词汇，提高模型的泛化能力。

### Unigram model (SentencePiece)

另一种新的分词方法：定义目标函数来捕捉更好的分词特征。