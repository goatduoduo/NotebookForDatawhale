# 引言

> 这里是朵朵的笔记，而不是照抄教程（这个用Ctrl C V三键的键盘就可以做到）
> 需要总结的是：学习到的知识总结，学习遇到的问题和对本次task的感受


该干活了（Liao）！

## 1.1 什么是语言模型

语言模型（Language Model）的经典定义是一种对令牌序列（token）的概率分布。

假设我们有一个令牌集的词汇表 $V$ 。语言模型p为每个令牌序列（又是token）分配一个 $0到1$ 概率。

$$
p(x_1, \dots, x_L)
$$

所以最后成为了一个概率问题，这个概率可以标记序列有多《好》。

例如，如果词汇表为{ate, ball, cheese, mouse, the}，语言模型可能会分配以下概率（演示）：

$$
p(\text{the, mouse, ate, the, lcheese}) = 0.02,
$$

$$
p(\text{the, cheese ate, the, mouse}) = 0.01,
$$

$$
p(\text{mouse, the, the, cheese, ate}) = 0.0001,
$$

这三句其实都有逻辑问题

- 第一句cheese打成了lcheese，但是语法是对的！
- 第二句主语和宾语都错了，芝士把老鼠吃了，太扯了！
- 第三句？？？你到底在说什么，该死！

**一句比一句离谱，分数一个比一个低！**

语言模型除了判断好坏以外，也可用于生成任务。

例如：从语言模型 $p$ 以概率 $p(x_{1:L})$ 进行采样，表示为：

$$
x_{1:L}∼p.
$$

通常来说，我们不会直接这么采样，受限于真实语言限制和追求最佳结果的追求。

### 自回归语言模型(Autoregressive language models)

逐渐开始看不懂了，好一个《简单》学点大模型

将序列   $x_{1:L}$  的联合分布  $p(x_{1:L})$  的常见写法是使用概率的链式法则：

$$
p(x_{1:L}) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) \cdots p(x_L \mid x_{1:L-1}) = \prod_{i=1}^L p(x_i \mid x_{1:i-1}).
$$

解释一下，这是一个连乘（一般会取对数防止下溢），而且还是个贝叶斯分布。

$$
\begin{align*} p({the}, {mouse}, {ate}, {the}, {cheese}) = \, & p({the}) \\ & p({mouse} \mid {the}) \\ & p({ate} \mid {the}, {mouse}) \\ & p({the} \mid {the}, {mouse}, {ate}) \\ & p({cheese} \mid {the}, {mouse}, {ate}, {the}). \end{align*}
$$

这样就好理解多了，连乘，下一个参数是给定前面的记号后，下一个记号的概率。

自回归语言模型的特点是它可以利用例如前馈神经网络等方法有效计算出每个条件概率分布  $p(x_{i}∣x_{1:i−1})$  。

所以仍然是要大量计算每个条件概率分布，实际上运算量是有些大的。

在非自回归任务中，要从自回归语言模型  $p$  中生成整个序列 $x_{1:L}$ ，我们需要一次生成一个令牌(token)，该令牌基于之前以生成的令牌进行计算获得：

$$
\begin{aligned}
\text { for } i & =1, \ldots, L: \\
x_i & \sim p\left(x_i \mid x_{1: i-1}\right)^{1 / T},
\end{aligned}
$$

最大的特点是引入了新的参数 $T(T≥0)$ 即温度，这是一个控制语言模型中得到多少随机性的温度参数：

- T=0：确定性地在每个位置 i 选择最可能的令牌 $x_{i}$
- T=1：从纯语言模型“正常（normally）”采样
- T=∞：从整个词汇表上的均匀分布中采样

但是，当 $T  = 0$ 时， $1/T$ 不就没有意义了？

对了，用极限的概念来理解他！

我们可以尝试一下不同的温度下概率会变得如何？当然会因为这个导致概率和不为 1，需要引入标准化分布来解决这个问题，标准化的分布也称为“退火条件概率分布”。例如：

$$
\begin{array}{cl}
p(\text { cheese })=0.4, & p(\text { mouse })=0.6 \\
p_{T=0.5}(\text { cheese })=0.31, & \left.p_{T=0.5} \text { (mouse }\right)=0.69 \\
\left.p_{T=0.2} \text { (cheese }\right)=0.12, & p_{T=0.2} \text { (mouse) }=0.88 \\
\left.p_{T=0} \text { (cheese }\right)=0, & \left.p_{T=0} \text { (mouse }\right)=1
\end{array}
$$

随着温度越来越接近0，两者差距越来越大！这个温度参数会应用于每一次条件概率分布，将其幂变为 $1/T$ 。这个值越高越接近平均概率分布，更高随机性，反之倾向于生成概率较高的token（更高准确度）。

对于非自回归的条件生成，通过指定某个前缀序列  $x_{1:i}$ （称为提示）并采样其余的  $x_{i+1:L}$ （称为补全）来进行条件生成。

the mouse ate (T=0) the cheese

当T=1时，就可以获得多样性，例如

the mouse ate (T=1) its house
the mouse ate (T=1) my homework

这就给了我们更改提示就能解决各种任务的能力。

## 1.2大模型相关历史回顾

### 1.2.1信息理论、英语的熵、n-gram模型

语言模型的发展可以追溯到克劳德·香农，在信息理论中再次引入了熵（Entropy）的概念：

$$
H(p) = \sum_x p(x) \log \frac{1}{p(x)}.
$$

熵实际上是衡量将样本压缩成比特串所需要的预期比特数数量，实际上就是代表的**信息量**。

举例来说，"the mouse ate the cheese" 可能会被编码成 "0001110101"。（不过怎么编码的我并不知道）

值越少说明序列的结构性越强，编码长度越短。（越说明这是个套话）

$\log \frac{1}{p(x)}$  可以视为用于表示出现概率为 $p(x)$ 的元素 $x$ 的编码的长度。

例如，如果 $p(x)=1/8$ ，我们就需要分配  $log_{2}(8)=3$ 个比特（或等价地， $log(8)=2.08$ 个自然单位）。

这也能理解，如果位数不够，是无法表示低概率的。

而追求香农极限会显得很有挑战性，这也是编码研究方向吧（我会干这个么？，也许）

#### 1.2.1.1英语的熵

香农特别对测量英语的熵感兴趣，将其表示为一系列的字母。这意味着我们想象存在一个“真实”的分布p（这种存在是有问题的，但它仍然是一个有用的数学抽象），它能产生英语文本样本x∼p。

香农还定义了交叉熵：

$$
H(p, q)=-\sum_x p(x) \log q(x)= \sum_x p(x) \log \frac{1}{q(x)}
$$

数学意义是，需要多少比特来编码样本x∼p，使用由模型q给出的压缩方案（用长度为1/q(x)的代码表示x）。

它的上界是熵H(p)

我们可以通过来自于真实数据的样本来估计 $H(p, q)$ 但是 $H(p)$ 通常无法访问。

因此，我们应该用 $H(p, q)$衡量 $H(p)$ 。

#### 1.2.1.2用于下游应用的N-gram模型

语音识别和机器翻译都使用了基于词的 n-gram 语言模型，最早由香农引入，但针对字符。

N-gram模型。在一个n-gram模型中，关于$x_{i}$的预测只依赖于最后的 $n-1$ 个字符 $x_{i−(n−1):i−1}$ ，而不是整个历史：

$$
p(x_i \mid x_{1:i-1}) = p(x_i \mid x_{i-(n-1):i-1}).
$$

例如，当 n=3 时
$$
p(𝖼𝗁𝖾𝖾𝗌𝖾∣𝗍𝗁𝖾,𝗆𝗈𝗎𝗌𝖾,𝖺𝗍𝖾,𝗍𝗁𝖾)=p(𝖼𝗁𝖾𝖾𝗌𝖾∣𝖺𝗍𝖾,𝗍𝗁𝖾)。
$$

也就是说，它取决于上文的前2个词（算上自己是3个）

这种模型只捕捉局部依赖关系，这就足够了，太短无法捕捉长距离依赖关系，太长则无法得到概率好的估计。

#### 1.2.1.3神经语言模型

在原先的语言模型中增加对神经网络的引入。其中 $p(x_{i}∣x_{i−(n−1):i−1})$ 由神经网络给出：

$$
p(cheese∣ate,the)=some-neural-network(ate,the,cheese)。
$$

注意，上下文长度仍然受到n的限制，但现在对更大的n值估计神经语言模型在统计上是可行的。

计算更加昂贵扩展性不如n-gram，但表现更优。

神经语言建模主要由两个关键发展，**Recurrent Neural Networks**和**Transformers**，n甚至达到了2048。