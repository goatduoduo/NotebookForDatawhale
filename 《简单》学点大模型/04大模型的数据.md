# 大语言模型背后的数据

大模型基于“原始文本”训练，因此为了实现它的能力就必须涵盖广泛的领域、类型、语言。

可能的文本来源：

- 网络：最少有10PB，而深网的规模比这还要大
- 公司：公司私有数据集，例如沃尔玛的数据量达到了2.5PB/H


但数据源仍然会存在问题

- 大规模数据在全球人口的代表性中不均衡
- 发达国家的年轻用户占比过大
- 性别和年龄的不平衡
- 对某些人群的歧视
- 过滤不良词汇导致某些人群被边缘化

### WebText和OpenWebText数据集

WebText数据集被用于训练GPT-2模型。其目标是获取既多样化又高质量的数据集。

- WebText数据集用于GPT-2模型的训练，旨在获取多样化和高质量的文本数据。之前的研究主要使用新闻、维基百科或小说等数据集进行训练，但Common Crawl包含了大量垃圾信息。

- Trinh和Le在2018年选择了Common Crawl的一小部分数据，根据n-gram与目标任务的重叠性，创建了WebText数据集。他们抓取了至少获得3个赞的所有外链，过滤掉维基百科内容，最终获得了40GB的文本数据。
- 尽管OpenAI没有公开发布WebText数据集，但OpenWebText数据集在理念上模仿了WebText的构建方法。OpenWebText并非WebText的副本，但它遵循了相似的思路和方法，以模拟WebText的数据特性，为研究提供了一种替代选择。
- OpenWebText数据集是从Reddit提交的数据中提取URL开始，然后使用fastText过滤非英语内容，并删除近乎重复的内容，最终得到了38GB的文本数据。
- 在2020年的RealToxicityPrompts研究中，对这两个数据集进行了毒性分析，发现OpenWebText中有2.1%的内容毒性得分>=50%，而WebText中有4.3%的内容毒性得分>=50%。此外，研究还发现新闻的可靠性与毒性呈负相关，并且OpenWebText中的一部分内容来自被禁止或被隔离的subreddits。

### Colossal Clean Crawled Corpus（C4）

- **C4语料库用于训练T5模型。** 该数据集源自2019年4月的Common Crawl快照，包含了大量的文本数据（1.4万亿个标记）。在准备过程中，移除了不良词汇、代码（例如“{”），并通过语言检测工具(langdetect)过滤掉了非英语文本，最终获得了806GB的文本数据（总计1560亿个标记）。

- **Dodge等人在2021年对C4数据集进行了深入分析。** 这项分析涵盖了多个方面，包括数据的元数据（来源和话语数据）、数据的内容（机器或人类创作、社会偏见、数据污染）以及数据的排除项（医疗或健康数据、人口身份等）。

- **关于C4数据集的独特性和问题：**
  - 其中一些数据来自于patents.google.com，占了相当大的比例。这些数据中，92%的页面是在过去十年内编写的，但令人惊讶的是，来自印度的页面数量相对较少，尽管印度有大量的英语使用者。
  - 部分文本是自动生成的，例如，外国官方语言（如日语）的专利被自动翻译成英语，还有一些文本是由光学字符识别（OCR）自动生成的。这可能导致文本中存在系统性的错误或奇怪的表现。

总之，C4语料库是一个用于训练T5模型的大规模文本数据集，但它也具有一些特殊的数据特点和问题，需要在使用时谨慎考虑。这些分析有助于研究人员更好地理解数据集的性质和潜在限制。

### Benchmark的数据污染问题

产生污染的原因在于训练数据和基准数据同源，很难做到完全分离。

而数据集本身也会引发问题，例如人类的偏见也会引发模型的偏见，数据的选择和过滤也会带来偏见本身。这对于大模型来说也是一种污染。

### GPT-3的数据集

- **GPT-3的数据集主要来自Common Crawl，类似于WebText。** GPT-3下载了Common Crawl数据的41个分片，这些数据跨足了2016年至2019年的时间范围。为了将WebText与Common Crawl区分开来，GPT-3使用了一个二元分类器，该分类器通过文档的特征来预测其接近WebText的概率，以此来决定是否保留文档。

- **在数据处理中，GPT-3采用了模糊去重的方法。** 这意味着它检测13-gram的重叠，如果某个窗口或文档中的13-gram在少于10个训练文档中出现，那么就会被移除。此外，GPT-3还从基准数据集中移除了一些数据，以提高数据质量。

- **GPT-3增加了数据来源的多样性。** 除了Common Crawl外，它还包括来自WebText2、Books1、Books2以及维基百科的数据。在训练过程中，Common Crawl的数据被降低了采样率，占整个数据集的82%，但只贡献了60%的数据。

- **EleutherAI推出了The Pile数据集。** 这个数据集的核心理念是从较小的高质量数据源（例如学术和专业资源）中获取数据，这与依赖网络爬虫不同，可以提供更高质量的数据用于训练语言模型。


### The Pile数据集

- **The Pile数据集**包含大量高质量数据，共825GB的英文文本，由22个数据集组成。这个数据集在训练GPT-2Pile模型时展示出了一些有趣的结果。

- **关于数据的总结**：虽然网络和私有数据的总量庞大，但简单地将所有数据用于训练可能并不是最有效的方法，特别是考虑到计算资源有限。因此，数据的过滤和策划（例如OpenWebText、C4、GPT-3数据集）是必要的，但可能会导致偏见。同时，开发非网络来源的高质量数据集（例如The Pile）是有潜力的，但也需要仔细的记录和审查，以确保数据的质量和可用性。这个问题强调了在训练语言模型时需要综合考虑数据质量、多样性和偏见问题。

## 数据集文档

数据文档非常重要，而目的有两个：让创建者反思他们的决策，如危害或偏见；让使用者了解何时用和不用。

数据集的生命周期中，需要考虑很多问题：

- 创建动力
- 创建者
- 资助者
- 各个组成部分
- 是否含有缺失
- 是否包含机密
- 如何获取
- 获取人员
  ……

## 数据生态

作者探讨了数据在不同领域中的角色和价值观，以及如何更好地管理、保护和利用数据。这些问题涵盖了数据管理、数据治理、数据尊严和数据伦理等多个方面。

- **数据的多维视角：** 文本指出，数据是一个广泛的概念，可以从不同角度进行研究。虽然在机器学习中通常将数据集视为固定的对象，但在数据库领域，有一整个子领域致力于研究数据如何构建和使用的生态系统，尤其在工业领域具有重要意义。

- **数据治理：** 数据治理关注组织如何创建、维护和确保数据的质量和安全性。BigScience项目的数据治理工作组正在开发框架，以负责任地规划高质量的数据源，而不是简单地爬取网页上的信息。

- **数据尊严：** 数据尊严是一个概念，强调数据不仅仅是个体的财产，而是群体的财产。这个概念反映了数据在集体层面上的重要性和价值。数据隐私关注的是个体层面的数据保护，但数据尊严强调了数据的集体价值。有提议建立数据联盟，这些联盟可以代表数据生产者进行集体谈判，以维护数据的尊严和价值。