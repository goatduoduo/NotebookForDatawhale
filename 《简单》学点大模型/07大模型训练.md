本章节关于"目标函数"和"优化算法"这两部分

## 目标函数

我们研究三类语言模型的目标函数:

1. 只包含解码器模型(GPT-3)：计算单向上下文嵌入，一次生成一个token
2. 之包含编码器（BERT）：计算双向上下文嵌入
3. 编码器解码器（T5）：编码输入，解码输出

我们可以使用任何模型将token序列映射到上下文嵌入中（例如，LSTM、Transformers）：

$$
\phi : V^L \to \mathbb{R}^{d \times L}. 
$$
$$
\left[\text{the}, \text{mouse}, \text{ate}, \text{the}, \text{cheese}\right] \stackrel{\phi}{\Rightarrow} \left[\binom{1}{0.1}, \binom{0}{1}, \binom{1}{1}, \binom{1}{-0.1}, \binom{0}{-1} \right].
$$

### Decoder-only 模型

自回归模型定义了条件分布
$$
p(x_i \mid x_{1:i-1}).
$$

#### 最大似然

设$\theta$是大语言模型的所有参数。设$D$是由一组序列组成的训练数据。
根据最大似然原理，定义负对数似然的目标函数。

$$
O(\theta) = \sum_{x \in D} - \log p_\theta(x) = \sum_{x \in D} \sum_{i=1}^L -\log p_\theta(x_i \mid x_{1:i-1}).
$$

嗯……仍然是简洁有力的公式

取了对数就可以有效应对连乘带来的下溢的问题。

### Encoder-only 模型

#### BERT

BERT的目标函数分为两大部分：

- 掩码语言模型
- 下一句预测

以自然语言推理（预测隐含、矛盾或中性）任务中的序列为例：

$$
x_{1:L} = [\text{[CLS]}, \text{all}, \text{animals}, \text{breathe}, \text{[SEP]}, \text{cats}, \text{breathe}].
$$

其中有两个特殊的token：
- $\text{[CLS]}$：包含用于驱动分类任务的嵌入
- $\text{[SEP]}$：用于告诉模型第一个序列（例如，前提）与第二个序列（例如，假设）的位置。

然后定义BERT模型：

$$
\text{BERT}(x_{1:L}) = \text{TransformerBlock}^{24}(\text{EmbedTokenWithPosition}(x_{1:L}) + \text{SentenceEmbedding}(x_{1:L})) \in \mathbb{R}^{d \times L},
$$

其实有一些坐飞机了，已经看不懂那是什么了。

其中，$\text{SentenceEmbedding}(x_{1:L})$根据序列返回以下两个矢量之一
- 对于$\text{[SEP]}$左边的，返回$e_A \in \mathbb{R}^d$
- 对于$\text{[SEP]}$右边的，返回$e_B \in \mathbb{R}^d$

BERT-large有$n_\text{heads} = 16$个注意头，并且$d_\text{model} = 1024$，总共355M个参数。

##### 掩码语言模型

基本思想是通过加“噪”然后预测来进行训练：

$$
[\text{the}, \text{[MASK]}, \text{ate}, \text{[MASK]}, \text{cheese}] \Rightarrow [\text{the}, \text{mouse}, \text{ate}, \text{the}, \text{cheese}].
$$

把一些词变成 [mask] 之后让大模型进行猜测……

我们可以将其视为去噪自动编码器，我们有带噪音的版本 $\tilde x_{1:L}$ ，然后尝试重建原始的 $x_{1:L}$ 。

$$
\tilde x_{1:L} \Rightarrow x_{1:L}.
$$

然后能看懂的止步于此……

这个模型建模，用A算法增加掩码，然后进行下一次预测。

而这个A算法是有讲究的，不过目前来说还是很难听懂。

##### 下一句预测

BERT在已经拼接好的句子上进行训练，下一句的预测目标是预测的第二句是否跟随第一句。

##### 数据集

$\mathcal{D}$是按如下方式构造的一组样本$(x_{1:L}, c)$：
- 令$A$是语料库中的一个句子。
- 以0.5的概率，$B$是下一句话。
- 以0.5的概率，$B$是语料库中的一个随机句子。
- 令$x_{1:L} = [\text{[CLS]}, A, \text{[SEP]}, B]$
- 令$c$表示$B$是否是下一句。

这就是让BERT做一个二分类任务，训练下一句是否是第一句跟随的。是不是牛头不对马嘴？

##### 训练目标

BERT的训练目标是：
$$
\mathcal{O}(\theta) = \sum_{(x_{1:L},c) \in \mathcal{D}} \underbrace{\mathbb{E}_{I, \tilde x_{1:L} \sim A(\cdot \mid x_{1:L}, I)}[\sum_{i \in I} -\log p_\theta(\tilde x_i \mid x_{1:L})]}_\text{masked language modeling} + \underbrace{-\log p(c \mid \phi(x_{1:L})_1)}_\text{next sentence prediction}.
$$

显然我看不懂这个是干什么的……

## Encoder-decoder 模型

该章节讲述了两种模型：

- BART(Bidirectional Auto-Regressive Transformers)
- T5 (Text-to-Text Transfer Transformer)

这两个模型对输入进行双向编码，对输出进行自回归编码。

同样的都使用了非常大的模型进行训练，效果都很好。

# 优化算法

## 随机梯度下降（SGD）

老朋友了，使用小批量参数进行梯度下降以快速收敛从而提高运算效率。

在很多的模型中都有应用。

## Adam (adaptive moment estimation)

这个算法引入了动量，参数的每个维度都有一个自适应不同的不长。

代价是参数量翻倍。

## AdaFactor
实际上是减少存储占用的优化算法，改变了存储策略并移除了动量。

它不储存$m_t,v_t$这样的$O(m \times n)$矩阵，而是存储行和列的和$O(m + n)$并重构矩阵

这样做的潜在代价是让训练变得更加困难

## 混合精度训练

使用更低精度的浮点来进行训练，使得存储容量减半。