本次比赛为数据挖掘类型的比赛，聚焦于工业场景。本赛题实质上为回归任务，其中会涉及到时序预测相关的知识。

通过电炉空间温度推测产品内部温度，设计烧结过程的温度场和浓度场的最优控制律：
- 任务输入：电炉对应17个温区的实际生产数据，分别是电炉上部17组加热棒设定温度T1-1 ~ T1-17，电炉下部17组加热棒设定温度T2-1~T2-17，底部17组进气口的设定进气流量V1-V17；
- 任务输出：电炉对应17个温区上部空间和下部空间17个测温点的测量温度值。


## 基线代码解读笔记

这是我第一次参加机器学习比赛，在参加比赛之前，我对代码一无所知，优化也无从谈起，所以只能从基线开始下手。

也许最近事情非常多，难以有效分配足够的时间来学习机器学习本身，讽刺的是这几个月的绝大多数时间就是为了学习“人工智能”，然后去转型。

我们选择了**lightGBM**作为机器学习算法，并且26日讲解了它的原理，并且知道了为什么比XGBoost的效率更高，内存占用更少。

## 使用到的库

- pandas 用于处理数据的工具，读取和加载csv文件，常用于数据加载、数据处理和数据预处理
- lightgbm 机器学习模型 LightGBM 构建梯度提升树模型，是高效的机器学习算法
- from sklearn.metrics import mean_absolute_error 评分 MAE 的计算函数，从sklearn模块中引入计分函数，平均绝对误差MAE是用于回归问题的一个评价指标
- from sklearn.model_selection import train_test_split 拆分训练集与验证集工具，用于将数据集拆分为训练集和测试集，也就是机器学习理论的留出法，以便进行模型训练和评估
- tqdm 显示循环的进度条工具，方便查看代码执行进度

这些库相当于“菜刀”，然后就可以开始切菜了！

## 数据准备

```python
# 数据准备
train_dataset = pd.read_csv("./data/train.csv") # 原始训练数据。
test_dataset = pd.read_csv("./data/test.csv") # 原始测试数据（用于提交）。

submit = pd.DataFrame() # 定义提交的最终数据。
submit["序号"] = test_dataset["序号"] # 对齐测试数据的序号。

MAE_scores = dict() # 定义评分项。
```

使用pandas读取数据，注意Linux路径和Windows路径的区别，win系统请使用“\\\\”而不是“\\”。

训练集的需要需要和测试集的序号对齐。

## 模型调参

```python
# 参数设置
pred_labels = list(train_dataset.columns[-34:]) # 需要预测的标签。
train_set, valid_set = train_test_split(train_dataset, test_size=0.2) # 拆分数据集。

# 设定 LightGBM 训练参，查阅参数意义：https://lightgbm.readthedocs.io/en/latest/Parameters.html
lgb_params = {
        'boosting_type': 'gbdt',
        'objective': 'regression',
        'metric': 'mae',
        'min_child_weight': 5,
        'num_leaves': 2 ** 5,
        'lambda_l2': 10,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 4,
        'learning_rate': 0.05,
        'seed': 2023,
        'nthread' : 16,
        'verbose' : -1,
    }

no_info = lgb.callback.log_evaluation(period=-1) # 禁用训练日志输出。
```

标签取最后34个，也就是上部17个温度和下部17个温度，即需要求的y

训练集和测试集按照4：1的方式拆分

lgb有很多的参数，往往有调参的空间

- 'boosting_type': 'gbdt' 使用的提升方法，梯度提升决策树gbdt，原理需要学习，无需改动
- 'objective': 'regression' 任务是回归，不需要改动
- 'metric': 'mae' 评估指标：MAE，平均误差
- 'min_child_weight': 5 子节点中的样本权重的最小和，控制过拟合
- 'num_leaves': 2 ** 5 叶子节点数，影响模型复杂度
- 'lambda_l2': 10 L2正则化项权重，用于控制复杂度
- 'feature_fraction': 0.8 随机选择特性比例，防止过拟合
- 'bagging_fraction': 0.8 随机选择数据比例，防止过拟合
- 'bagging_freq': 4 随机选择数据频率，防止过拟合
- 'learning_rate': 0.05 学习率，也就是迭代的步长，过高可能难以收敛
- 'seed': 2023 随机种子
- 'nthread' : 16 并行线程数，无需改动
- 'verbose' : -1 控制日志输出，-1不输出日志

分类、然后设定参数


## 特性提取

```python
# 时间特征函数
def time_feature(data: pd.DataFrame, pred_labels: list=None) -> pd.DataFrame:
    """提取数据中的时间特征。

    输入: 
        data: Pandas.DataFrame
            需要提取时间特征的数据。

        pred_labels: list, 默认值: None
            需要预测的标签的列表。如果是测试集，不需要填入。
    
    输出: data: Pandas.DataFrame
            提取时间特征后的数据。
    """
    
    data = data.copy() # 复制数据，避免后续影响原始数据。
    data = data.drop(columns=["序号"]) # 去掉”序号“特征。
    
    data["时间"] = pd.to_datetime(data["时间"]) # 将”时间“特征的文本内容转换为 Pandas 可处理的格式。
    data["month"] = data["时间"].dt.month # 添加新特征“month”，代表”当前月份“。
    data["day"] = data["时间"].dt.day # 添加新特征“day”，代表”当前日期“。
    data["hour"] = data["时间"].dt.hour # 添加新特征“hour”，代表”当前小时“。
    data["minute"] = data["时间"].dt.minute # 添加新特征“minute”，代表”当前分钟“。
    data["weekofyear"] = data["时间"].dt.isocalendar().week.astype(int) # 添加新特征“weekofyear”，代表”当年第几周“，并转换成 int，否则 LightGBM 无法处理。
    data["dayofyear"] = data["时间"].dt.dayofyear # 添加新特征“dayofyear”，代表”当年第几日“。
    data["dayofweek"] = data["时间"].dt.dayofweek # 添加新特征“dayofweek”，代表”当周第几日“。
    data["is_weekend"] = data["时间"].dt.dayofweek // 6 # 添加新特征“is_weekend”，代表”是否是周末“，1 代表是周末，0 代表不是周末。

    data = data.drop(columns=["时间"]) # LightGBM 无法处理这个特征，它已体现在其他特征中，故丢弃。

    if pred_labels: # 如果提供了 pred_labels 参数，则执行该代码块。
        data = data.drop(columns=[*pred_labels]) # 去掉所有待预测的标签。
    
    return data # 返回最后处理的数据。

test_features = time_feature(test_dataset) # 处理测试集的时间特征，无需 pred_labels。
test_features.head(5)
```

去掉“序号”的特征，把时间拆分成Pandas可处理的格式，lightGBM只能处理int整数类型，丢弃原先的时间特性

特性工程非常重要，到底输出数据取决于什么呢？我们可以在这里做更多文章，然后尝试寻找哪里相关。


### 训练和预测
``` python
# 从所有待预测特征中依次取出标签进行训练与预测。
for pred_label in tqdm(pred_labels):
    # print("当前的pred_label是：", pred_label)
    train_features = time_feature(train_set, pred_labels=pred_labels) # 处理训练集的时间特征。
    # train_features = enhancement(train_features_raw)
    train_labels = train_set[pred_label] # 训练集的标签数据。
    # print("当前的train_labels是：", train_labels)
    train_data = lgb.Dataset(train_features, label=train_labels) # 将训练集转换为 LightGBM 可处理的类型。

    valid_features = time_feature(valid_set, pred_labels=pred_labels) # 处理验证集的时间特征。
    # valid_features = enhancement(valid_features_raw)
    valid_labels = valid_set[pred_label] # 验证集的标签数据。
    # print("当前的valid_labels是：", valid_labels)
    valid_data = lgb.Dataset(valid_features, label=valid_labels) # 将验证集转换为 LightGBM 可处理的类型。

    # 训练模型，参数依次为：导入模型设定参数、导入训练集、设定模型迭代次数（200）、导入验证集、禁止输出日志
    model = lgb.train(lgb_params, train_data, 750, valid_sets=valid_data, callbacks=[no_info])

    valid_pred = model.predict(valid_features, num_iteration=model.best_iteration) # 选择效果最好的模型进行验证集预测。
    test_pred = model.predict(test_features, num_iteration=model.best_iteration) # 选择效果最好的模型进行测试集预测。
    MAE_score = mean_absolute_error(valid_pred, valid_labels) # 计算验证集预测数据与真实数据的 MAE。
    MAE_scores[pred_label] = MAE_score # 将对应标签的 MAE 值 存入评分项中。

    submit[pred_label] = test_pred # 将测试集预测数据存入最终提交数据中。
     
submit.to_csv('submit_result.csv', index=False) # 保存最后的预测结果到 submit_result.csv
```

调用time_feature来处理时间特征，返回数据集

将train_labels和train_features转换为lightGBM可处理的类型，创建训练数据集train_data

验证集也需要做相同的事情，生成valid_features，valid_labels，然后也要转化成lightGBM可处理的类型

这些都是训练的准备工作，直接调用lgb.train开始训练，有训练集和验证集，还有迭代次数。我可能会考虑看训练日志，看看到底训练个什么。

训练出了model之后，就可以对验证集和测试集进行预测了，然后我们可以评价验证集的MAE来判断训练的效果是否可以，可作为参考。

最后保存测试集，那就是我们训练的最后结果了。

代码有了大致的理解了。

## 可能的调参优化方式

清洗数据

进一步的特征工程

数据决定模型上限，参数基本已经非常好了，继续优化的方式可能是对数据进行调整，可视化、删除不必要的参数、提取特征？