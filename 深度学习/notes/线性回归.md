# 线性回归的基本元素
 线性回归基于几个简单的假设： 首先，假设自变量和因变量之间的关系是线性的， 即可以表示为中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。
 
有一个场景，影响房价的关键因素是卧室个数，卫生间个数和居住面积，分别记作$x_1$,$x_2$,$x_3$

且成交价是关键因素的加权和，则可记作

$y=w_1x_1+w_2x_2+w_3x_3+b$

# 线性模型

* 给定n维输入 $x = [x_1,x_2,...,x_n]^T$
* 线性模型有一个n维权重和一个标量偏差 $w = [w_1,w_2,...,w_n]^T$
* 输出是输入的加权和 $y=w_1x_1+w_2x_2+...+w_nx_n+b$
* 向量版本： $y = <w,x>+b$
* 其中w是权重（weight），b是偏置（bias）

线性模型可以看做是单层神经网络

“所以如果到了多层又是什么情况呢？”

**神经网络源于神经科学**
# 损失函数

- 比较真实值和预估值，例如房屋售价和估价
- 假设$y$是真实值，$\hat y$是估计值，我们可以比较
- $\ell (y,y\hat) = {1 \over 2}{(y - y\hat)^2}$
- 这个叫做平方损失
- “二分之一是为了在求导的时候方便消去使用”

# 训练数据
- 收集一些数据点来决定参数值 (权重和偏差)，例如过去6个月卖的房子
- 这被称之为训练数据
- 通常越多越好
- 假设我们有n个样本，记
$$X = {[{x_1},{x_2}, \ldots ,{x_n}]^T}~~y = {[{y_1},{y_2}, \ldots ,{y_n}]^T}$$

训练损失
$$\ell (X,y,w,b) = {1 \over {2n}}\sum\limits_{i = 1}^n {{{({y_i} - \langle {x_i},w\rangle  - b)}^2} = } {1 \over {2n}}{\left\| {y - Xw - b} \right\|^2}$$

目标自然是找到w和b使得**损失最小**
$${w^*},{b^*} = \arg \mathop {\min }\limits_{w,b} \ell (X,y,w,b)$$

# 显示解
这个解可以通过一个公式表达出来，就是解析解。

我们将偏置合并到参数中，合并方法是在包含所有参数的矩阵中附加一列。 我们的预测问题是最小化$\left\| {y - Xw} \right\|^2$。

损失是凸函数，所以最优解满足
$${\partial  \over {\partial w}}\ell (X,y,w) = 0$$
$$ \Leftrightarrow {1 \over n}{(y - Xw)^T}X = 0$$
$$ \Leftrightarrow {w^*} = {({X^T}X)^{ - 1}}Xy$$

唯一的一个有最优解的模型，以后都不会有了……可惜

# 总结
- 线性回归是对n维输入的加权，外加偏差
- 使用平方损失来衡量预测值和真实值的差异
- 线性回归有显示解
- 线件回归可以看做是单层神经网络