# 线性模型

给定 $d$ 个属性的示例 $x$ ，线性模型试图学得一个通过属性的线性组合进行预测的函数
$$f(x)=w_1x_1+w_2x_2+...+w_dx_d+b$$

用向量形式写成

$$
y = w^Tx + b
$$

线性模型简单且易于建模，并且可以衍生出更为强大的非线性模型，并且 $w$ 可以直观表达出各属性的重要性。

例如，判断一个西瓜是否为好瓜，可以用如下表达式来判断：
$$
f_{好瓜}(x) = 0.2x_{色泽} + 0.5x_{根蒂} + 0.3x_{敲声} + 1
$$

在这里，根蒂就成为了最重要的值，敲声其次，色泽就没那么重要了。

## 线性回归

为了使用线性回归，需要将”非数值类属性“转化为数值类。

对于离散属性，如果存在“序”关系的属性，可以转化为连续值。例如身高的“高”和“矮”可以取 $\{1.0，0.0\}$ ,如果不存在，转化为k维向量，例如“瓜类”的取值“西瓜”、“南瓜”、“黄瓜”可以转化为 $(0,0,1),(0,1,0),(0,0,1)$

### 均方误差

最常用的衡量性能的度量，求出一组 $w^*,b^*$ 使得误差最小化，均方误差取得其实就是误差的平方和。

$$
(w^*, b^*) = arg min \sum_{i=1}^{m}{(f(x_i) - y_i)^2} \\
 = arg min \sum_{i=1}^{m}{(y_i - wx_i - b)^2}
$$

求解 $w,b$ 使得上述式子最小化的过程，称为线性回归模型的最小二乘“参数估计”。

线性回归是唯一一个可以求出闭式解“解析解”的，做法是分别对 $w$ 和 $b$ 求偏导，令导数为0可以求得最优解，当然我没必要放计算过程，可以直接看结果。

$$ w=\frac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}{x_i}^2-\frac{1}{m}({\sum_{i=1}^{m}{x_i})^2} }  $$

$$ b=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i) $$

推广到d个属性描述，此时试图学得

$$
f(x_i) = w^Tx_i + b
$$

这就成为了“多元线性回归”，也可以用最小二乘法进行估计，把 $w$ 和 $b$ 吸收入向量形式 $ \hat{w} =(w;b) $，将最小二乘法运用在一元线性回归上可得

$$
\hat{w}^* =arg_{\hat{w}}min(y-X\hat{w})^T(y-X\hat{w})
$$

为了损失函数最小，我们也做同样的操作，对 $\hat{w}$ 求导，然后令导数为0就可以得到最优的 $\hat{w}$

$$ \frac{\partial E_{\hat{w}}}{\partial\hat{w}}=2X^T(X\hat{w}-y) $$

这必然是复杂的运算，笔记里暂时不讨论这么复杂的运算情况。

### 对数几率回归

旨在使用线性模型进行分类任务

将y转换为0/1值就可以进行分类了，但是线性预测值 $z=w^T+b$ 是实数，转化最理想的方法是“单位阶跃函数”

$$
y=\begin{cases}
0, z<0\\
0.5, z=0\\
1, z>0
\end{cases}
$$

大于0判正，小于0判负，临界值随即判别

但是由于单位阶跃函数不连续，因此出现了“对数几率函数”作为常用的替代函数，它是“Sigmoid”函数，将 z转化为一个很接近0或1的y值，并且在z=0附近变化很陡。

$$ 
y=\frac{1}{1+e^{-z}}
$$

因此可以得出它为正例的几率y，和为反例的几率1-y

$ \frac{y}{1-y} $ 被称为几率，反映了x作为正例的相对可能性，取对数就可以得到“对数几率” $ ln\frac{y}{1-y} $

使用对数几率回归的一般流程是：

在训练集上学得模型

$$
y=\frac{1}{1+e^{-(w^TX+b)}}
$$

对于新样本 $x$ ，代入模型获得 $y_i$ ，设定阈值，一般为0.5，超过时判正例，否则反例。

通过最大似然法来估计w和b，对率回归模型最大化“对数似然”

$$ \ell (w,b)=\sum_{i=1}^{m}lnp(y_i|x_i,b)  $$

令每个样本属于其真实标记的概率越大越好，所以叫做“最大似然估计”

令 $\beta =(w;b),\hat{x}=(x;1)$ 可以得到……


……

总之整个过程非常复杂，涉及到牛顿法、梯度下降等理论，公式到现在没有完全看懂，还是复杂的很。

需要反复阅读这里，要推导还是比较困难的。

## 线性判别分析

从几何分析的角度，让全体训练样本经过投影后做到

- 异类样本中心尽可能远
- 同类样本的方差尽可能小

![这是图片](/img/LDA.png "P-R")

给定数据集 $D=\{(x_i,y_i)\}_{i=1}^m, y=\{0,1\}$

- $X_i$ 是示例的集合
- $\mu_i$ 均值向量
- $\sum_i$ 协方差矩阵

两类样本的中心在直线 $w$ 上的投影分别为 $w^T\mu_0$ 和 $w^T\mu_1$

两类样本协方差分别为 $w^T\sum_0w$ 和 $w^T\sum_1w$

要做到异类样本尽可能远，即 $max||w^T\mu_0-w^T\mu_1||_2^2$

同类样本方差尽可能小，即 $min(w^T\sum_0w+w^T\sum_1w)$

推导损失函数为 $max J=\frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\sum_0w+w^T\sum_1w}=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\sum_0+\sum_1)w}$

求解过程用到了拉格朗日乘子法

令 $S_w=\sum_0+\sum_1$ 和 $S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$

重写为 $J=\frac{w^TS_bW}{w^TS_wW}$

公式的解与 $w$ 的长度无关，只与方向有关，因此可以令 $w^TS_wW=1$，等价于

$$ \underset{w}{min} = -w^TS_bw $$

$$ s.t. w^TS_wW=1 $$

用拉格朗日乘子法，等价于

$$ S_bw=\lambda S_ww $$

还是一样，仍然需要时间消化公式，依然很难