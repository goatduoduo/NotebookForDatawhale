## 决策树

决策树基于树结构进行分类和决策
- 从逻辑角度，一堆if else语句的组合
- 从几何角度，根据某种准则划分特征空间
- 最终目的是为了把样本分的越来越纯


用机器学习算法根据最有可能的特征进行划分，如果分类不纯可以继续按照其他特征来划分，当然仍然会有很多问题，例如分类效果不好或者过拟合等，所以算法设计时需要考虑这些。

## 划分选择算法

**ID3决策树**-基于信息增益划分的决策树

“信息熵”用于衡量样本的纯度，假定样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k$ ,那么信息熵定义为

$$ Ent(D)=-\sum_{k=1}^{y}p_k\log_2p_k$$

值越少，信息熵越高，也就是纯度越高

如果 $p(x)=0$ ,则 $p_k\log_2p_k=0$

当某个样本的概率为1时信息熵最小，上述式子=0

当每个样本概率均等时信息熵最大，上述式子为 $\log_b|X|$ 其中 $|X|$为个数

信息增益是：在已知属性（特征） $a$ 的取值后 $y$ 的不确定性减少的量，即纯度提升

$$ Gain(D,a) = Ent(D)-\sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)$$

$$ \frac{|D^v|}{|D|}指的是该类型的占比$$

实际上是：信息熵-条件熵

ID3决策树根据信息增益最大的那个作为划分依据，然后继续下去

### C4.5决策树

使用增益率不完全代替信息增益，是一种启发式的方法：先选**信息增益**高于平均水平的属性，然后选择**增益率**最高的。

$$ Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$

其中 $IV(a)$ 表示的是a属性的固有值，$a$ 的取值个数 $V$ 越大，通常其固有值 $IV(a)$ 也越大。但是增益率可能对取值数目较少的属性有偏好。

### CART决策树

区别与前两种决策树，它引入了**基尼值**，而不是纯度本身！

基尼值：从样本集合D中随机抽取两个样本，其标记不一致的几率。基尼值越小，碰到异类的概率越小，纯度越高。

$$ Gini(D)=1-\sum_{k=1}^{|y|}p_k^2 $$

属性a的基尼指数

$$ Gini\_index(D,a)=\sum_{v=1}^{V}\frac{|D_v|}{|D|}Gini(D^v)$$

该决策树选择基尼指数最小的属性作为最优划分属性

实际构造算法
- 对每个属性 $a$ 的每个可能取值 $v$ ，将数据集 $D$ 分为 $a=v$ 和 $a!=v$ 两部分算基尼指数 $Gini\_index(D,a)=\frac{|D^{a=v}|}{|D|}Gini(D^{a=v})+\frac{|D^{a!=v}|}{|D|}Gini(D^{a!=v})$
- 按照基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点
- 重复以上两步，直至满足停止条件

